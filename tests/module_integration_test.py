#!/usr/bin/env python3
"""
ğŸ§ª PulseBridge.ai Module Integration Testing Suite
Advanced testing framework for validating modular architecture

This test suite validates:
âœ… All module imports work correctly
âœ… Router integrations are functional  
âœ… Cross-module dependencies resolve
âœ… Database connections are stable
âœ… API endpoints respond correctly
"""

import sys
import os
import asyncio
import importlib
import traceback
from typing import List, Dict, Any, Optional
from datetime import datetime
import json

# Add the app directory to the Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'app'))

class ModuleTestingFramework:
    """Advanced testing framework for module integration validation"""
    
    def __init__(self):
        self.results = {
            "timestamp": datetime.now().isoformat(),
            "tests_run": 0,
            "tests_passed": 0,
            "tests_failed": 0,
            "modules_tested": [],
            "import_results": {},
            "router_results": {},
            "endpoint_results": {},
            "dependency_map": {},
            "performance_metrics": {},
            "recommendations": []
        }
        
        # Define our expected module structure
        self.expected_modules = {
            "ai_endpoints": {"router": "ai_router", "prefix": "/api/v1"},
            "ai_chat_service": {"exports": ["ai_service", "ChatRequest"]},
            "optimization_endpoints": {"router": "optimization_router"},
            "sync_endpoints": {"router": "sync_router"}, 
            "analytics_endpoints": {"router": "analytics_router"},
            "autonomous_decision_endpoints": {"router": "autonomous_router"},
            "hybrid_ai_endpoints": {"router": "hybrid_ai_router"},
            "meta_business_api": {"exports": ["meta_api"]},
            "linkedin_ads_integration": {"functions": ["get_linkedin_ads_status", "get_linkedin_ads_campaigns"]},
            "pinterest_ads_integration": {"functions": ["get_pinterest_ads_status", "get_pinterest_ads_campaigns"]},
            "meta_ai_hybrid_integration": {"classes": ["PulseBridgeAIMasterController", "CrossPlatformMetrics"]},
            "smart_risk_management": {"classes": ["SmartRiskManager", "ClientReportingManager"]},
            "google_ads_integration": {"functions": ["get_google_ads_client"]}
        }

    def print_header(self, title: str):
        """Print a styled header for test sections"""
        print(f"\n{'='*60}")
        print(f"ğŸ§ª {title}")
        print(f"{'='*60}")

    def print_success(self, message: str):
        """Print success message with green checkmark"""
        print(f"âœ… {message}")

    def print_failure(self, message: str, error: Optional[str] = None):
        """Print failure message with red X"""
        print(f"âŒ {message}")
        if error:
            print(f"   Error: {error}")

    def print_info(self, message: str):
        """Print info message with blue icon"""
        print(f"â„¹ï¸  {message}")

    def test_module_imports(self) -> Dict[str, Any]:
        """Test all module imports systematically"""
        self.print_header("MODULE IMPORT TESTING")
        
        import_results = {}
        
        for module_name, config in self.expected_modules.items():
            self.results["tests_run"] += 1
            
            try:
                # Attempt to import the module
                start_time = datetime.now()
                module = importlib.import_module(module_name)
                import_time = (datetime.now() - start_time).total_seconds()
                
                # Validate expected exports exist
                validation_results = self._validate_module_exports(module, module_name, config)
                
                import_results[module_name] = {
                    "status": "success",
                    "import_time": import_time,
                    "validation": validation_results,
                    "module_path": getattr(module, "__file__", "unknown")
                }
                
                self.print_success(f"{module_name} imported successfully ({import_time:.3f}s)")
                self.results["tests_passed"] += 1
                
            except Exception as e:
                import_results[module_name] = {
                    "status": "failed",
                    "error": str(e),
                    "traceback": traceback.format_exc()
                }
                self.print_failure(f"{module_name} import failed", str(e))
                self.results["tests_failed"] += 1
        
        self.results["import_results"] = import_results
        return import_results

    def _validate_module_exports(self, module, module_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """Validate that expected exports exist in the module"""
        validation = {"exports_found": [], "exports_missing": [], "unexpected_exports": []}
        
        # Check for router
        if "router" in config:
            router_name = config["router"]
            if hasattr(module, router_name):
                validation["exports_found"].append(router_name)
            else:
                validation["exports_missing"].append(router_name)
        
        # Check for functions
        if "functions" in config:
            for func_name in config["functions"]:
                if hasattr(module, func_name):
                    validation["exports_found"].append(func_name)
                else:
                    validation["exports_missing"].append(func_name)
        
        # Check for classes
        if "classes" in config:
            for class_name in config["classes"]:
                if hasattr(module, class_name):
                    validation["exports_found"].append(class_name)
                else:
                    validation["exports_missing"].append(class_name)
        
        # Check for specific exports
        if "exports" in config:
            for export_name in config["exports"]:
                if hasattr(module, export_name):
                    validation["exports_found"].append(export_name)
                else:
                    validation["exports_missing"].append(export_name)
        
        return validation

    def test_router_integrations(self) -> Dict[str, Any]:
        """Test that all routers can be integrated with FastAPI"""
        self.print_header("ROUTER INTEGRATION TESTING")
        
        router_results = {}
        
        # Import FastAPI to test router compatibility
        try:
            from fastapi import FastAPI
            test_app = FastAPI(title="Test App")
            
            for module_name, config in self.expected_modules.items():
                if "router" not in config:
                    continue
                    
                self.results["tests_run"] += 1
                router_name = config["router"]
                
                try:
                    # Import the module and get the router
                    module = importlib.import_module(module_name)
                    router = getattr(module, router_name)
                    
                    # Test router integration
                    prefix = config.get("prefix", "")
                    if prefix:
                        test_app.include_router(router, prefix=prefix)
                    else:
                        test_app.include_router(router)
                    
                    # Count routes in the router
                    route_count = len(router.routes) if hasattr(router, 'routes') else 0
                    
                    router_results[module_name] = {
                        "status": "success",
                        "router_name": router_name,
                        "prefix": prefix,
                        "route_count": route_count,
                        "router_type": str(type(router))
                    }
                    
                    self.print_success(f"{module_name}.{router_name} integrated ({route_count} routes)")
                    self.results["tests_passed"] += 1
                    
                except Exception as e:
                    router_results[module_name] = {
                        "status": "failed",
                        "error": str(e),
                        "traceback": traceback.format_exc()
                    }
                    self.print_failure(f"{module_name}.{router_name} integration failed", str(e))
                    self.results["tests_failed"] += 1
            
        except Exception as e:
            self.print_failure("FastAPI import failed", str(e))
            return {"error": "FastAPI not available"}
        
        self.results["router_results"] = router_results
        return router_results

    def test_dependency_resolution(self) -> Dict[str, Any]:
        """Test cross-module dependencies"""
        self.print_header("DEPENDENCY RESOLUTION TESTING")
        
        dependency_map = {}
        
        for module_name in self.expected_modules.keys():
            self.results["tests_run"] += 1
            
            try:
                module = importlib.import_module(module_name)
                
                # Analyze module dependencies by examining imports
                dependencies = self._analyze_module_dependencies(module_name)
                
                dependency_map[module_name] = {
                    "status": "success",
                    "dependencies": dependencies,
                    "circular_dependencies": self._check_circular_dependencies(module_name, dependencies)
                }
                
                self.print_success(f"{module_name} dependencies resolved ({len(dependencies)} deps)")
                self.results["tests_passed"] += 1
                
            except Exception as e:
                dependency_map[module_name] = {
                    "status": "failed",
                    "error": str(e)
                }
                self.print_failure(f"{module_name} dependency resolution failed", str(e))
                self.results["tests_failed"] += 1
        
        self.results["dependency_map"] = dependency_map
        return dependency_map

    def _analyze_module_dependencies(self, module_name: str) -> List[str]:
        """Analyze what other modules this module depends on"""
        dependencies = []
        
        try:
            # Read the module file to analyze imports
            module_path = os.path.join(os.path.dirname(__file__), '..', 'app', f'{module_name}.py')
            if os.path.exists(module_path):
                with open(module_path, 'r') as f:
                    content = f.read()
                    
                # Look for imports of our other modules
                for other_module in self.expected_modules.keys():
                    if other_module != module_name:
                        if f"from {other_module}" in content or f"import {other_module}" in content:
                            dependencies.append(other_module)
        
        except Exception:
            pass  # Ignore file reading errors
            
        return dependencies

    def _check_circular_dependencies(self, module_name: str, dependencies: List[str]) -> List[str]:
        """Check for circular dependency patterns"""
        circular = []
        
        for dep in dependencies:
            if dep in self.expected_modules:
                dep_dependencies = self._analyze_module_dependencies(dep)
                if module_name in dep_dependencies:
                    circular.append(dep)
        
        return circular

    async def test_database_connectivity(self) -> Dict[str, Any]:
        """Test database connectivity across modules"""
        self.print_header("DATABASE CONNECTIVITY TESTING")
        
        db_results = {}
        
        try:
            # Import main to get Supabase connection
            import main
            
            if hasattr(main, 'SUPABASE_AVAILABLE') and main.SUPABASE_AVAILABLE:
                self.print_success("Supabase connection available")
                
                # Test basic database operations
                if hasattr(main, 'supabase') and main.supabase:
                    try:
                        # Test a simple query
                        result = main.supabase.table("leads").select("id").limit(1).execute()
                        
                        db_results = {
                            "status": "success",
                            "connection_available": True,
                            "test_query_successful": True,
                            "supabase_client": str(type(main.supabase))
                        }
                        
                        self.print_success("Database test query successful")
                        self.results["tests_passed"] += 1
                        
                    except Exception as e:
                        db_results = {
                            "status": "partial",
                            "connection_available": True,
                            "test_query_successful": False,
                            "error": str(e)
                        }
                        
                        self.print_failure("Database test query failed", str(e))
                        self.results["tests_failed"] += 1
                
            else:
                db_results = {
                    "status": "unavailable",
                    "connection_available": False,
                    "reason": "SUPABASE_AVAILABLE = False"
                }
                
                self.print_info("Database connection not available (expected in testing)")
                
        except Exception as e:
            db_results = {
                "status": "failed",
                "error": str(e)
            }
            
            self.print_failure("Database connectivity test failed", str(e))
            self.results["tests_failed"] += 1
        
        self.results["tests_run"] += 1
        return db_results

    def generate_recommendations(self) -> List[str]:
        """Generate recommendations based on test results"""
        recommendations = []
        
        # Check import success rate
        import_success_rate = (
            len([r for r in self.results["import_results"].values() if r["status"] == "success"]) /
            len(self.results["import_results"]) * 100
        ) if self.results["import_results"] else 0
        
        if import_success_rate < 100:
            recommendations.append(f"Import success rate is {import_success_rate:.1f}%. Consider fixing failed imports.")
        
        # Check for circular dependencies
        circular_deps = []
        for module, data in self.results.get("dependency_map", {}).items():
            if data.get("circular_dependencies"):
                circular_deps.extend(data["circular_dependencies"])
        
        if circular_deps:
            recommendations.append(f"Circular dependencies detected: {circular_deps}. Consider refactoring.")
        
        # Check router integration
        router_failures = [
            module for module, data in self.results.get("router_results", {}).items()
            if data.get("status") == "failed"
        ]
        
        if router_failures:
            recommendations.append(f"Router integration failures: {router_failures}. Check FastAPI compatibility.")
        
        # Performance recommendations
        slow_imports = [
            module for module, data in self.results.get("import_results", {}).items()
            if data.get("import_time", 0) > 0.1
        ]
        
        if slow_imports:
            recommendations.append(f"Slow imports detected: {slow_imports}. Consider optimizing module loading.")
        
        if not recommendations:
            recommendations.append("ğŸ‰ All tests passing! Module integration is excellent.")
            recommendations.append("âœ¨ Consider adding performance monitoring for production.")
            recommendations.append("ğŸš€ Ready for advanced features and scaling.")
        
        self.results["recommendations"] = recommendations
        return recommendations

    def print_summary(self):
        """Print a comprehensive test summary"""
        self.print_header("TEST SUMMARY REPORT")
        
        # Overall statistics
        total_tests = self.results["tests_run"]
        passed_tests = self.results["tests_passed"]
        failed_tests = self.results["tests_failed"]
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        print(f"ğŸ“Š Total Tests: {total_tests}")
        print(f"âœ… Passed: {passed_tests}")
        print(f"âŒ Failed: {failed_tests}")
        print(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
        
        # Module status
        print(f"\nğŸ“¦ Module Status:")
        for module_name in self.expected_modules.keys():
            import_status = self.results["import_results"].get(module_name, {}).get("status", "unknown")
            router_status = self.results["router_results"].get(module_name, {}).get("status", "n/a")
            
            status_icon = "âœ…" if import_status == "success" else "âŒ"
            print(f"   {status_icon} {module_name}: import={import_status}, router={router_status}")
        
        # Recommendations
        print(f"\nğŸ’¡ Recommendations:")
        for rec in self.results["recommendations"]:
            print(f"   â€¢ {rec}")
        
        # Performance insights
        if self.results["import_results"]:
            avg_import_time = sum(
                data.get("import_time", 0) for data in self.results["import_results"].values()
            ) / len(self.results["import_results"])
            print(f"\nâš¡ Average Import Time: {avg_import_time:.3f}s")

    def save_results(self, filename: str = "module_test_results.json"):
        """Save detailed test results to a JSON file"""
        try:
            with open(filename, 'w') as f:
                json.dump(self.results, f, indent=2, default=str)
            self.print_success(f"Test results saved to {filename}")
        except Exception as e:
            self.print_failure(f"Failed to save results", str(e))

    async def run_all_tests(self):
        """Run the complete test suite"""
        self.print_header("ğŸš€ PULSEBRIDGE.AI MODULE INTEGRATION TEST SUITE")
        print(f"Starting comprehensive testing at {datetime.now().isoformat()}")
        
        # Run all test phases
        self.test_module_imports()
        self.test_router_integrations()
        self.test_dependency_resolution()
        await self.test_database_connectivity()
        
        # Generate insights
        self.generate_recommendations()
        
        # Display results
        self.print_summary()
        self.save_results()
        
        return self.results

async def main():
    """Main test execution function"""
    framework = ModuleTestingFramework()
    results = await framework.run_all_tests()
    
    # Exit with appropriate code
    exit_code = 0 if results["tests_failed"] == 0 else 1
    
    if exit_code == 0:
        print(f"\nğŸ‰ ALL TESTS PASSED! Module integration is rock solid! ğŸš€")
    else:
        print(f"\nâš ï¸  Some tests failed. Check the results above for details.")
    
    return exit_code

if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)